{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time E-commerce Data Pipeline with Spark ETL\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will design and implement an ETL (Extract, Transform, Load) pipeline for a hypothetical e-commerce platform named **ShopEase**. The platform generates massive amounts of data daily, including user interactions, transactions, and inventory updates. Your task is to process this data using Apache Spark to derive meaningful insights and support real-time analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "**ShopEase** aims to enhance its data analytics capabilities to improve user experience, optimize inventory management, and increase sales. The data generated includes:\n",
    "\n",
    "- **User Activity Logs:** Clickstream data capturing user interactions on the website.\n",
    "- **Transaction Records:** Details of purchases, refunds, and returns.\n",
    "- **Inventory Updates:** Information about stock levels, restocks, and product information changes.\n",
    "- **Customer Feedback:** Reviews and ratings provided by customers.\n",
    "\n",
    "The company requires a robust ETL pipeline to process this data, perform transformations, and make it available for analytics and reporting in both batch and real-time modes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Requirements\n",
    "\n",
    "You are required to perform the following tasks using Apache Spark (preferably with PySpark or Scala):\n",
    "\n",
    "### 1. Data Ingestion\n",
    "- **Batch Data:**\n",
    "  - Load historical data from large CSV and JSON files stored in your local file system.\n",
    "- **Real-Time Data:**\n",
    "  - Simulate and ingest streaming data from a Kafka topic representing live user activity logs.\n",
    "\n",
    "### 2. Data Processing and Transformation\n",
    "- **Using RDDs:**\n",
    "  - Perform a transformation to filter out any corrupted or incomplete records from the transaction data.\n",
    "  - Implement a custom transformation to anonymize user IDs for privacy compliance.\n",
    "- **Using DataFrames:**\n",
    "  - Clean and standardize inventory data (e.g., handling missing values, normalizing text).\n",
    "  - Join user activity logs with transaction records to analyze user behavior leading to purchases.\n",
    "- **Using Spark SQL:**\n",
    "  - Create temporary views and execute SQL queries to compute:\n",
    "    - Top 10 most purchased products in the last month.\n",
    "    - Monthly revenue trends.\n",
    "    - Inventory turnover rates.\n",
    "\n",
    "### 3. Real-Time Streaming Processing (Optional but Recommended)\n",
    "- Set up a Spark Streaming job to process incoming user activity logs.\n",
    "- Compute real-time metrics such as:\n",
    "  - Active users per minute.\n",
    "  - Real-time conversion rates.\n",
    "  - Detect and alert on unusual spikes in specific user activities.\n",
    "\n",
    "### 4. Data Storage\n",
    "- Store the transformed data into appropriate storage systems:\n",
    "  - Use Parquet format for batch-processed data in a local data lake.\n",
    "  - Use an in-memory data store like Redis or a database like PostgreSQL for real-time metrics.\n",
    "\n",
    "### 5. Performance Optimization\n",
    "- Optimize Spark jobs for better performance by:\n",
    "  - Caching intermediate DataFrames where necessary.\n",
    "  - Tuning Spark configurations (e.g., partition sizes, executor memory).\n",
    "  - Using appropriate join strategies.\n",
    "\n",
    "### 6. Documentation and Reporting\n",
    "- Document the ETL pipeline architecture.\n",
    "- Provide sample dashboards or reports (using Spark's built-in visualization) showcasing the insights derived.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions & Requirements\n",
    "\n",
    "### 1. Data Ingestion\n",
    "- **Q1:** How did you ingest the batch and real-time data? Provide code snippets demonstrating the loading of data using RDDs and DataFrames.\n",
    "\n",
    "### 2. Data Cleaning and Transformation\n",
    "- **Q2:** Describe the transformations applied to the transaction data using RDDs. How did you ensure data quality and privacy?\n",
    "\n",
    "### 3. DataFrame Operations\n",
    "- **Q3:** How did you clean and standardize the inventory data using DataFrames? Provide examples of handling missing values and normalizing text fields.\n",
    "\n",
    "### 4. Spark SQL Queries\n",
    "- **Q4:** Present the Spark SQL queries used to calculate the top 10 most purchased products, monthly revenue trends, and inventory turnover rates.\n",
    "\n",
    "### 5. Real-Time Processing\n",
    "- **Q5:** If implemented, explain how the real-time streaming was set up. What metrics were computed in real-time, and how were they stored/displayed?\n",
    "\n",
    "### 6. Performance Optimization\n",
    "- **Q6:** What strategies did you employ to optimize the performance of your Spark jobs? Provide examples of configuration settings or code optimizations.\n",
    "\n",
    "### 7. Reporting\n",
    "- **Q7:** Show sample outputs or dashboards that visualize the insights derived from the ETL pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Ingestion - Load Large Datasets\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = \n",
    "\n",
    "# Load large transactions data\n",
    "transactions_df =\n",
    "\n",
    "# Load large inventory data\n",
    "inventory_df =\n",
    "\n",
    "# Load large customer feedback data\n",
    "feedback_df = \n",
    "\n",
    "# Display a sample of each dataset\n",
    "transactions_df.show(5)\n",
    "inventory_df.show(5)\n",
    "feedback_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Cleaning and Transformation with RDDs\n",
    "# Convert transactions DataFrame to RDD\n",
    "transactions_rdd = \n",
    "\n",
    "# Filter out corrupted records (e.g., missing transaction_id or amount)\n",
    "cleaned_rdd = \n",
    "\n",
    "# Write Function to Anonymize user IDs using Hashing\n",
    "def anonymize(record):\n",
    "    \n",
    "\n",
    "anonymized_rdd = \n",
    "\n",
    "# Convert back to DataFrame\n",
    "cleaned_transactions_df = \n",
    "\n",
    "# Display cleaned and anonymized data\n",
    "cleaned_transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: DataFrame Operations for Cleaning and Transformation\n",
    "from pyspark.sql.functions import col, lower, trim\n",
    "\n",
    "# Clean inventory data by handling missing values and normalizing text\n",
    "cleaned_inventory_df = inventory_df.dropna(subset=[\"stock_level\"]) \\\n",
    "                                  .withColumn(\"product_name\", lower(trim(col(\"product_name\"))))\n",
    "\n",
    "# Display cleaned inventory data\n",
    "cleaned_inventory_df.show(5)\n",
    "\n",
    "# Perform a join operation to combine data\n",
    "joined_df = \n",
    "\n",
    "# Display joined DataFrame\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Spark SQL Queries\n",
    "# Create temporary views for SQL queries\n",
    "cleaned_transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "cleaned_inventory_df.createOrReplaceTempView(\"inventory\")\n",
    "joined_df.createOrReplaceTempView(\"joined_data\")\n",
    "\n",
    "# Query: Top 10 most purchased products in the last month\n",
    "top_products_df = \n",
    "\n",
    "top_products_df.show()\n",
    "\n",
    "# Query: Monthly revenue trends\n",
    "monthly_revenue_df = \n",
    "\n",
    "monthly_revenue_df.show()\n",
    "\n",
    "# Query: Inventory turnover rates\n",
    "turnover_rate_df = \n",
    "turnover_rate_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Real-Time Processing (Optional)\n",
    "from pyspark.sql.functions import window, countDistinct\n",
    "\n",
    "# For demonstration, create a streaming DataFrame from a sample batch dataset\n",
    "streaming_transactions_df = spark.readStream.schema(transactions_df.schema) \\\n",
    "                                           .csv(\"streaming_transactions_folder\")  # Point to a folder with incoming data\n",
    "\n",
    "# Compute real-time metrics (e.g., active users per minute)\n",
    "active_users = \n",
    "\n",
    "# Display active users in real-time (Note: This will print continuously if run with actual streaming data)\n",
    "query = \n",
    "\n",
    "query.awaitTermination()  # Keep the stream running (can be stopped manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Performance Optimization Techniques\n",
    "# Caching DataFrames to optimize performance for multiple transformations\n",
    "\n",
    "\n",
    "# Repartition DataFrames for optimal join performance\n",
    "transactions_df_repartitioned = \n",
    "inventory_df_repartitioned =\n",
    "\n",
    "# Use Broadcast Join for small DataFrames (if applicable)\n",
    "joined_df_optimized = \n",
    "\n",
    "# Display the optimized joined DataFrame\n",
    "joined_df_optimized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Store the Transformed Data\n",
    "# Store the cleaned and transformed data in Parquet format\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dashboard Outputs\n",
    "- **Top Products Bar Chart:** Displaying the top 10 products with the highest sales.\n",
    "- **Revenue Trend Line Chart:** Showing monthly revenue over the past year.\n",
    "- **Inventory Turnover Heatmap:** Visualizing turnover rates across different product categories.\n",
    "\n",
    "*(Include actual screenshots or detailed descriptions as appropriate.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Top Products Bar Chart - Displaying the top 10 products with the highest sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Revenue Trend Line Chart - Showing monthly revenue over the past yea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inventory Turnover Heatmap - Visualizing turnover rates across different product categories\n",
    "Assuming you have a product_category column in the inventory data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guidelines\n",
    "- Ensure that your code is well-documented and follows best practices.\n",
    "- Include instructions on how to set up and run your ETL pipeline.\n",
    "- Provide all necessary configurations and dependencies required for execution.\n",
    "- If using external services (like Kafka or Redis), include setup instructions or mock implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- [Apache Spark Documentation](https://spark.apache.org/documentation.html)\n",
    "- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Optimizing Spark Performance](https://spark.apache.org/docs/latest/tuning.html)\n",
    "- [Using Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
